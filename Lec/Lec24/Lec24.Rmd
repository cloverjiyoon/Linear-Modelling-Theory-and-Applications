---
title: "Lec24"
author: "Jiyoon Clover Jeong"
date: "11/29/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(faraway)
library(rpart)
```


```{r}

#Ozone dataset from the library Faraway: 

data(ozone, package = "faraway")
help(ozone)
dd = ozone
head(ozone)
pairs(dd)
#O3 is the response variable. The rest are explanatory variables. 
#Recursive Partitioning algorithm
rt = rpart(O3 ~ ., data = dd)
plot(rt, margin = 0.1)
text(rt)

```

```{r}

#understanding the output
rt
#If the clause at the beginning of a node is met, we go left and if it is not met, we go right. 
#The depth of the branches is proportional to the reduction in error due to the split. The TSS here is 21115.4100:
sum((dd$O3 - mean(dd$O3))^2)
mean(dd$O3)

```


```{r}

#The deviance of the two groups corresponding to the split temp < 67.5 equals 4114.3040 and 5478.44.
#The reduction in error due to this split is (4114.3040 + 5478.44)/21115.41 = 0.4543006. 
#This is substantial reduction compared to the subsequent reductions and hence is plotted by a big branch. 

ind =  which(dd$temp  < 67.5)
mean(dd$O3[ind])
sum((dd$O3[ind] - mean(dd$O3[ind]))^2)
length(ind)
sum((dd$O3[-ind] - mean(dd$O3[-ind]))^2)

ind2 =  which(dd$ibh[ind] >= 3573.5)
 length(ind2)
mean(dd$O3[ind[ind2]])


#If you want to plot the tree with uniform spacing, use
plot(rt, compress = T, uniform = T, branch = 0.4, margin = 0.1)
text(rt)


```


```{r}
#The tree generated by rt has been constructed from a large tree by cost-complexity pruning
printcp(rt)


#This gives the best cross-validated error for various values of cp = alpha. The term relative error : simply RSS(T)/TSS. ----> RSS(T) only counts ternimal nodes

```


```{r}



#Q : This will therefore always decrease as the tree complexity increases. The xerror is an error calculated by 10-fold cross-validation (and divided by TSS). 

#Because the partition of data into 10 parts is random, xerror is random and xstd provides its standard deviation.


#The tree generated by rpart corresponds to the cp for which xerror is the smallest. If we think that the tree generated by rpart is too big, we can prune it by selecting our own value of cp:
#Using xerror and xstd, we can choose a higher value of cp for a more interpretable tree if we so desire.
rta = prune.rpart(rt, 0.026756) 
#quartz()
plot(rta, margin = 0.1)
text(rta)
#The regression tree can be compared, for example, with a linear model. 

#Using a smaller value of cp: 
rt = rpart(O3 ~ ., data = dd, cp = 0.001)
plot(rt, margin = 0.1)
text(rt)
printcp(rt)
rta = prune.rpart(rt, 0.02315)
#quartz()
plot(rta, margin = 0.1)
text(rta)
#Note that xerror is random. 

#"R^2" for regression tree:
1 - (sum(residuals(rta)^2))/(sum((ozone$O3 - mean(ozone$O3))^2))


```



```{r}

fit <- lm(formula = siri ~ age + weight + height + knee + biceps + wrist, data = fat)
summary(fit)

head(fat)

```